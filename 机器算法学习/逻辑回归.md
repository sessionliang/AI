# 逻辑回归 Logistic Regression

逻辑回归算法是做分类任务的，并且通常用于二分类任务。也可以使用多个逻辑回归算法组合起来做多分类预测。

## 1. 逻辑回归的理解

逻辑回归的分类预测分为两个步骤：
1. 拟合决策边界，找到线性回归函数f(x)
2. f(x)对输入x做预测，结果做归一化处理，求得概率

### 1.1 拟合决策边界

我们通过天池实验室的[机器学习算法（一）: 基于逻辑回归的分类预测]('https://developer.aliyun.com/ai/scenario/9ad3416619b1423180f656d1c9ae44f7')中的3.1 Demo实践章节来理解一下上面的两个步骤。

首先展示一下样本分布，以及用来分类的决策边界。

```
# 数据样本，分类标签只有0和1，特征值两个x1, x2
x_fearures = np.array([[-1, -2], [-2, -1], [-3, -2], [1, 3], [2, 1], [3, 2]])
y_label = np.array([0, 0, 0, 1, 1, 1])
```

![逻辑回归-决策边界](images/逻辑回归-决策边界.png)

上图中，紫色点为分类0，黄色点为分类1。假设一个函数：
$$f(x) = w0 + w1 \times x1 + w2 \times x2$$
那么对于新样本有下面两种情况(忽略样本点落在f(x)上的情况)：

1. 当f(x) > 0时，样本点会落在蓝色线的上方，也就是分类为1；
2. 当f(x) < 0时，样本点会落在蓝色线线的下方，也就是分类为0。

那么函数f(x) = 0就是本次分类试验的决策边界。

当我们把一个函数通过训练样本，最终拟合成决策边界函数，那么通过这个函数对新输入样本x的输出，就可以作为分类判断的依据。这个过程也就是开始时候说的逻辑回归第一个步骤：拟合决策边界，找到线性回归函数f(x)。

### 1.2 归一化

既然通过第一个步骤就可以对样本进行分类了，那么为什么还需要对上面线性回归的结果做归一化呢？有下面几个原因：

1. 线性回归预测的值f(x) ∈ R, 是连续的；但是分类的预测结果却是0和1，是离散的；
2. 线性回归预测的值f(x)，只能表示属于哪个分类。归一化之后的结果可以当做分类概率，更好的阐释分类结果。

逻辑函数中的归一化采用了sigmoid函数：
$$g(z) = \frac{1}{1 + e^{(-z)}}$$
$$z = f(x)$$

sigmoid函数又叫做s型函数，如下图：

![逻辑回归-sigmoid](images/逻辑回归-sigmoid.png)

从图中观察，我们可以得到：
当x越大，y值越趋近于1；如果把这里的x换做线性回归的预测值的话，就是预测值越大，通过sigmoid得到的概率越接近于1，这个概率就是P(y=1)的概率；
当x越小，y值越趋近于0；也就是说线性回归的预测值越小，通过sigmoid得到的概率越小，P(y=1)越小，那么P(y=0)越大。

所以，通过sigmoid函数做归一化的结果符合我们的预期，函数值在0~1之间，可以很好的洽和概率的分布。

那么最终的预测模型为：
$$y = \frac{1}{1 + e^{-(w0 + w1 \times x1 + w2 \times x2)}}$$

上面的分析是从一个简单的二分类问题，引出决策边界的概念，然后在将线性回归的结果通过sigmoid函数转换为概率输出。

### 1.3 逻辑回归的数学认知

分类如果从概率角度看，可以设定分类为1的概率是y, 那么分类为0的概率就是1-y。y称为正例概率，1-y称为反例概率。他们两者的比值被称为几率o：
$$o = \frac{y}{1-y}$$

从公式就可以看出来：
1. 当样本分类为1的时候，y > 1-y，几率o > 1;
2. 当样本分类为0的时候，y < 1-y，几率o < 1;

函数中有一类函数，输入x正好是以1为分界线，输出y发生了正负变化，这类函数就是对数函数。上面的几率就可以认为是对数函数的输入，样本的分类就可以认为是对数函数的输出。

![逻辑回归-对数几率函数](images/逻辑回归-对数几率函数.png)

已知条件：

$$
\begin{aligned}
y &= \frac{1}{1 + e^{-k}}\\\\
k &= w0 + w1 \times x1 + w2 \times x2\\\\
\end{aligned}
$$

对数几率函数做推导：

$$
\begin{aligned}
\ln\frac{y}{1-y} &= \ln{y} - \ln(1-y)\\\\
&=-\ln({1+e^{-k}}) - \ln({1-\frac{1}{1+e^{-k}}})\\\\
&=-\ln({1+e^{-k}}) - \ln{e^{-k}} + \ln({1+e^{-k}})\\\\
&=-\ln{e^{-k}}\\\\
&=k
\end{aligned}
$$

对数几率函数最终的推导结果，其实就是决策边界函数，从这里也可以看出来，决策边界函数输出的结果正负情况对应的就是分类的结果1和0.

## 2. 逻辑回归函数求参

这个部分讨论的其实就是怎么从给定的训练样本，求对数几率函数的参数问题。也就是说逻辑回归模型是怎么训练的？

在机器学习中，求解最优解问题，有一个比较通用的模式是：
1. 假设一个模型函数，初始化一套参数
2. 针对模型函数，提出他的损失函数
3. 求损失函数最小时候的参数，也就是模型的最优解

上面的这个过程，在线性回归和逻辑回归中是通用的，不同的地方在于损失函数是不一样的。接下来我们逐步分析一下模型函数，损失函数，以及求最优解。

### 2.1 模型函数
待补充
### 2.2 损失函数
待补充
### 2.3 求最优解
待补充

## 3. 逻辑函数的代码实现

待补充

## 参考资料
1. [markdown中数学公式书写](https://www.jianshu.com/p/8b6fc36035c0)
2. [机器学习-逻辑回归](https://zhuanlan.zhihu.com/p/74874291)
3. [github中显示数学公式](https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima/related)